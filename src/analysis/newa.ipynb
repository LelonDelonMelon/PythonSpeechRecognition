{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "# Set the path to the Librispeech dataset\n",
    "\n",
    "trainData_path = \"../../LibriSpeech/train-clean\"\n",
    "testData_path = \"../../LibriSpeech/test-clean\"\n",
    "json_path_train = \"../json/train_data.json\"\n",
    "json_path_test = \"../json/test_data.json\"\n",
    "# Set the desired user IDs\n",
    "user_ids = [19, 26, 32, 27, 39, 78, 40, 405, 83, 196]\n",
    "num_classes = len(user_ids)\n",
    "\n",
    "# Set the parameters for audio feature extraction\n",
    "sample_rate = 16000  # Adjust according to your dataset\n",
    "n_mfcc = 13  # Number of MFCC coefficients\n",
    "frame_length = 0.025  # Length of each frame in seconds\n",
    "frame_stride = 0.01  # Length of stride between frames in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import json\n",
    "# Preprocess audio data and extract features (MFCCs)\n",
    "def preprocess_audio(audio_path, sample_rate=16000, n_mfcc=13, frame_length=0.025, frame_stride=0.01):\n",
    "    # Load audio file\n",
    "    audio, _ = librosa.load(audio_path, sr=sample_rate)\n",
    "    \n",
    "    # Extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc,\n",
    "                                 hop_length=int(frame_stride * sample_rate),\n",
    "                                 n_fft=int(frame_length * sample_rate))\n",
    "    \n",
    "    # Normalize MFCCs (optional)\n",
    "    mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
    "    \n",
    "    # Transpose MFCCs to have the shape (num_frames, num_mfcc, 1)\n",
    "    mfccs = mfccs.T\n",
    "    \n",
    "    return mfccs.tolist()\n",
    "\n",
    "def prepare_data(pathToDataset,jsonPath):\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "\n",
    "    for user_id in user_ids:\n",
    "        user_dir = os.path.join(pathToDataset, f\"{user_id}\")\n",
    "        for file_name in os.listdir(user_dir):\n",
    "            audio_path = os.path.join(user_dir, file_name)\n",
    "            features = preprocess_audio(audio_path)\n",
    "            if features is not None:  # Skip audio files with errors\n",
    "                data[\"mfcc\"].append(features)\n",
    "                data[\"labels\"].append(user_ids.index(user_id))\n",
    "    \n",
    "    # Could be NumPy Array\n",
    "    with open(jsonPath ,\"w\") as fp:\n",
    "            json.dump(data,fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "import tensorflow as tf\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Reshape(input_shape),  # Reshape the input to expected shape\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, training_data, training_labels, validation_data, validation_labels, batch_size=32, epochs=10):\n",
    "    model.fit(\n",
    "        training_data,\n",
    "        training_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(validation_data, validation_labels)\n",
    "    )\n",
    "    # Evaluate the model\n",
    "def evaluate_model(model, test_data, test_labels):\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "    print(\"Test Loss:\", loss)\n",
    "    print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data and save them as JSON file\n",
    "prepare_data(trainData_path,json_path_train)\n",
    "prepare_data(testData_path,json_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    with open(dataset_path,\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    # convert lists into numpy arrays\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "   \n",
    "    targets = np.array(data[\"labels\"])\n",
    "    return inputs, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "inputs, targets = load_data(json_path_train)\n",
    "# Train Test Split\n",
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs,\n",
    "                                                                          targets,\n",
    "                                                                          test_size= 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bunun numpy Arrayi olması gerekiyor, ama bu bildigin python listi, bunu yarın çözcez\n",
    "inputs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "input_shape = (inputs.shape[0])  # Shape of a single sample\n",
    "model = build_model(input_shape, num_classes)\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model,  train_splitted_data, train_splitted_label, validation_data, validation_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
